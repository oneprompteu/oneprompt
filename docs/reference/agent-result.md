# AgentResult

Every SDK method (`query`, `chart`, `analyze`) returns an `AgentResult` object containing the operation's output, metadata, generated files, and execution metrics.

```python
result = client.query("Top 10 products by revenue")

print(result.ok)          # True
print(result.summary)     # "Top 10 products by revenue"
print(result.preview)     # [{"product_name": "Widget Pro", ...}, ...]
print(result.artifacts)   # [ArtifactRef(...), ...]
print(result.metrics)     # RunMetrics(duration_ms=4821.3, input_tokens=1240, ...)
```

---

## Properties

| Property | Type | Description |
|----------|------|-------------|
| `ok` | `bool` | Whether the operation succeeded |
| `run_id` | `str` | Unique identifier for this execution |
| `session_id` | `str` | Session this execution belongs to |
| `summary` | `str \| None` | Human-readable summary of the result |
| `data` | `dict` | Agent-specific response fields (see below) |
| `artifacts` | `list[ArtifactRef]` | Generated files (CSV, JSON, HTML) |
| `metrics` | `RunMetrics \| None` | Token usage and timing (local mode only) |
| `error` | `str \| None` | Error message if `ok` is `False` |

### Convenience Properties

| Property | Type | Description |
|----------|------|-------------|
| `preview` | `list[dict]` | Preview rows from data queries. Shortcut for `data["preview"]` |
| `columns` | `list[str]` | Column names from data queries. Shortcut for `data["columns"]` |

---

## Usage Examples

### Checking success

```python
result = client.query("Revenue by month")

if result.ok:
    print(result.summary)
    for row in result.preview:
        print(row)
else:
    print(f"Error: {result.error}")
```

### Accessing preview data

```python
result = client.query("Top 5 customers")

# Column names
print(result.columns)  # ["name", "total_spend"]

# Data rows
for row in result.preview:
    print(f"{row['name']}: ${row['total_spend']}")
```

### Working with artifacts

```python
result = client.query("Monthly revenue")

for artifact in result.artifacts:
    print(f"File: {artifact.name}")
    print(f"Type: {artifact.type}")

# Read content on demand (fetched from the Artifact Store)
csv_text = result.artifacts[0].read_text()

# Save to a local file
result.artifacts[0].download("./output/")
```

### Inspecting execution metrics

```python
result = client.query("Revenue by month")

m = result.metrics
print(f"Ran in {m.duration_ms / 1000:.1f}s")
print(f"Tokens: {m.input_tokens} in / {m.output_tokens} out")
print(f"LLM calls: {m.llm_calls}")
```

### Error handling

```python
result = client.query("Revenue from nonexistent_table")

if not result.ok:
    print(f"Failed: {result.error}")
    # "Failed: Query failed: relation 'nonexistent_table' does not exist"
```

### Chaining results

```python
data = client.query("Sales by category")
chart = client.chart("Pie chart of category distribution", data_from=data)
```

See [Chaining Agents](../guides/chaining.md) for more patterns.

---

## Agent Response Data

The `data` property contains agent-specific fields. Redundant fields (`ok`, `error`, `artifacts`) and internal Docker paths are stripped automatically â€” only informative fields are kept.

### Data Agent

```python
result.data = {
    "intent": "export",           # "preview" or "export"
    "columns": ["name", "revenue"],
    "preview": [{"name": "Widget", "revenue": "1000.00"}],
    "row_count": 10,              # None if not counted
    "format": "json",
}
```

### Chart Agent

```python
result.data = {
    "tool": "chart_bar",          # chart tool used
    "name": "bar_chart.json",     # output file name
}
```

### Python Agent

```python
result.data = {}  # summary is promoted to result.summary; artifacts to result.artifacts
```

---

## RunMetrics

`result.metrics` is a `RunMetrics` object populated in local mode. It accumulates token usage across all LLM calls made during the agent run.

```python
from oneprompt import RunMetrics
```

### Fields

| Field | Type | Description |
|-------|------|-------------|
| `duration_ms` | `float` | Wall-clock time for the full agent run (milliseconds) |
| `input_tokens` | `int` | Total input tokens sent to the LLM |
| `output_tokens` | `int` | Total output tokens generated by the LLM |
| `total_tokens` | `int` | Sum of input and output tokens |
| `reasoning_tokens` | `int \| None` | Thinking/reasoning tokens (models with extended thinking) |
| `cached_tokens` | `int \| None` | Tokens served from the provider's prompt cache |
| `llm_calls` | `int` | Number of individual LLM roundtrips (one per agent step) |

### Example

```python
result = client.query("Summarize sales by region")

m = result.metrics
# RunMetrics(duration_ms=6123.4, input_tokens=2100, output_tokens=418, total_tokens=2518, llm_calls=3)

print(f"Duration:  {m.duration_ms / 1000:.1f}s")
print(f"Input:     {m.input_tokens} tokens")
print(f"Output:    {m.output_tokens} tokens")
print(f"Total:     {m.total_tokens} tokens")
if m.reasoning_tokens:
    print(f"Reasoning: {m.reasoning_tokens} tokens")
if m.cached_tokens:
    print(f"Cached:    {m.cached_tokens} tokens")
print(f"LLM calls: {m.llm_calls}")
```

!!! note "Cloud mode"
    `metrics` is `None` when using cloud mode (`oneprompt_api_key` is set), since agent execution happens server-side.
